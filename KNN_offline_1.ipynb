{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06229cffe52a28d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:58:15.916292408Z",
     "start_time": "2023-09-11T08:58:14.547580672Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import lightgbm as lightgbm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774b3fa314830ccf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:58:25.445948931Z",
     "start_time": "2023-09-11T08:58:25.072937054Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/data/Soheil-data/Python Projects/KNN-LETOR/valid_svm_features_avg.pkl', 'rb') as f:\n",
    "    preprocessed_train_cum_features = pickle.load(f) #main dict train with qids and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46687be3ff90a798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:58:34.036498085Z",
     "start_time": "2023-09-11T08:58:33.884522513Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/data/Soheil-data/Python Projects/KNN-LETOR/test_svm_features_avg.pkl', 'rb') as f:\n",
    "    preprocessed_test_cum_features = pickle.load(f) #main dict with test qids and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e882cc594d0082",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T09:00:35.262256708Z",
     "start_time": "2023-09-11T08:58:38.874160692Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/data/Soheil-data/Python Projects/KNN-LETOR/valid_svm_features_rel.pkl', 'rb') as f:\n",
    "    train_data_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c452daa59f3064c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T09:02:39.039024364Z",
     "start_time": "2023-09-11T09:00:35.263272580Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/data/Soheil-data/Python Projects/KNN-LETOR/test_svm_features_rel.pkl', 'rb') as f:\n",
    "    test_data_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a13e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_cum_features= pd.DataFrame(preprocessed_train_cum_features)\n",
    "preprocessed_test_cum_features= pd.DataFrame(preprocessed_test_cum_features)\n",
    "train_data_all= pd.DataFrame(train_data_all)\n",
    "test_data_all= pd.DataFrame(test_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091ef9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_avg_train = preprocessed_train_cum_features[\"q_id\"].tolist()\n",
    "features_avg_train = preprocessed_train_cum_features[\"features\"].tolist()\n",
    "qid_avg_train = np.vstack(qid_avg_train)\n",
    "features_avg_train = np.vstack(features_avg_train)\n",
    "\n",
    "qid_avg_test = preprocessed_test_cum_features[\"q_id\"].tolist()\n",
    "features_avg_test = preprocessed_test_cum_features[\"features\"].tolist()\n",
    "qid_avg_test = np.vstack(qid_avg_test)\n",
    "features_avg_test = np.vstack(features_avg_test)\n",
    "\n",
    "qid_all_train= train_data_all[\"q_id\"].tolist()\n",
    "relevance_all_train = train_data_all[\"relevance\"].tolist()\n",
    "features_all_train = train_data_all[\"features\"].tolist()\n",
    "qid_all_train = np.vstack(qid_all_train)\n",
    "relevance_all_train = np.vstack(relevance_all_train)\n",
    "features_all_train = np.vstack(features_all_train)\n",
    "\n",
    "qid_all_test = test_data_all[\"q_id\"].tolist()\n",
    "relevance_all_test = test_data_all[\"relevance\"].tolist()\n",
    "features_all_test = test_data_all[\"features\"].tolist()\n",
    "\n",
    "qid_all_test = np.vstack(qid_all_test)\n",
    "relevance_all_test = np.vstack(relevance_all_test)\n",
    "features_all_test = np.vstack(features_all_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae33fdc73b16fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58bc081ddfb506",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "#find the nearest neighbour\n",
    "#key2: features\n",
    "#key1: qids\n",
    "def get_nearest_neighbour(test_q, features_avg_train,k):\n",
    "    knn = NearestNeighbors(n_neighbors=k)\n",
    "    features_avg_train = np.array(features_avg_train)\n",
    "    \n",
    "    knn.fit(features_avg_train)\n",
    "    test_q = test_q.reshape((1, -1))\n",
    "    dist_loc = knn.kneighbors(test_q)\n",
    "    dist = dist_loc[0]\n",
    "    loc = dist_loc[1]\n",
    "    return dist, loc\n",
    "#these have relevance score, qids, and features\n",
    "#key_0 = relevance score, key_qids, key2_features list\n",
    "\"\"\"\n",
    "test_unique_qids = []    # a list for getting the qids of tests\n",
    "for d in qid_avg_test:\n",
    "    if d not in test_unique_qids:\n",
    "        test_unique_qids.append(d)\n",
    "\"\"\"\n",
    "k= 300\n",
    "#qids_test = []\n",
    "train_ids_group =[]\n",
    "#len(qid_all_train)\n",
    "for i in range(0,5):\n",
    "    print(i)\n",
    "    #get features corresponding to the qids of the test\n",
    "    #qids_test.append(qid)\n",
    "    #test_q = #[d['features'] for d in preprocessed_train_cum_features if d.get('q_id') == key1_value]\n",
    "    #get the nearest neighbour of the test_q\n",
    "    train_feature_single = features_avg_train[i]\n",
    "    \n",
    "    #features_avg_test = features_avg_test.reshape((features_avg_test.shape[0],-1))\n",
    "    \n",
    "    features_avg_train = features_avg_train.reshape((features_avg_train.shape[0], 220))\n",
    "    \n",
    "    dist , loc = get_nearest_neighbour(train_feature_single, features_avg_train,k)\n",
    "    #get the values of the features of the trainset\n",
    "    selected_train_set = features_avg_train[loc]\n",
    "    #get the nearest query ids and add them to this list\n",
    "    matching_train_qids = qid_avg_train[loc]\n",
    "    matching_train_qids = matching_train_qids.reshape(-1,1)\n",
    "    new_train_features = []\n",
    "    new_train_qids = []\n",
    "    new_train_relevances = []\n",
    "    for qid in matching_train_qids:\n",
    "        train_data_ids = np.where(qid_all_train == qid)[0]\n",
    "        for k in range(0, len(train_data_ids)):\n",
    "            new_train_qids.append(qid_all_train[train_data_ids[k]])\n",
    "            new_train_features.append(features_all_train[train_data_ids[k]])\n",
    "            new_train_relevances.append(relevance_all_train[train_data_ids[k]])\n",
    "    \n",
    "    new_train_relevances = np.array(new_train_relevances)\n",
    "    new_train_features = np.array(new_train_features)\n",
    "    new_train_qids = np.array(new_train_qids)\n",
    "    \n",
    "    #add k-neareset ids to a list of ids \n",
    "    train_ids_group.append(matching_train_qids.flatten())\n",
    "    # Reshape \"relevance\" and \"qid\" arrays\n",
    "    new_train_relevances = new_train_relevances.reshape(-1)\n",
    "    new_train_qids = new_train_qids.reshape(-1)\n",
    "    #new_train_features = new_train_features.tolist()\n",
    "    # Concatenate arrays and list\n",
    "    #train_data = np.column_stack((new_train_relevances, new_train_qids, new_train_features))\n",
    "    data_train_tmp = {\n",
    "    'relevance': new_train_relevances,\n",
    "    'qid': new_train_qids,\n",
    "    'features': new_train_features.tolist()  # Convert features to a list of lists\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame\n",
    "    train_group = pd.DataFrame(data_train_tmp)\n",
    "    \n",
    "    qids_train = train_group.groupby(\"qid\")[\"qid\"].count().to_numpy()\n",
    "    #qids_test = test_group.groupby(\"qid\")[\"qid\"].count().to_numpy()\n",
    "    \n",
    "    new_train_relevances = new_train_relevances.astype(int)\n",
    "    #new_test_relevances = new_test_relevances.astype(int)\n",
    "    a = time.time()\n",
    "    ranker = lightgbm.LGBMRanker(\n",
    "                        objective=\"lambdarank\",\n",
    "                        boosting_type = \"gbdt\",\n",
    "                        n_estimators = 1,\n",
    "                        importance_type = \"gain\",\n",
    "                        force_col_wise=True,\n",
    "                        metric= \"ndcg\",\n",
    "                        num_leaves = 250,\n",
    "                        learning_rate = 0.05,\n",
    "                        max_depth = -1,\n",
    "                        device ='gpu',\n",
    "                        label_gain =[i for i in range((new_train_relevances.max()) + 1)])\n",
    "    \n",
    "    # Training the model\n",
    "    ranker.fit(\n",
    "          X=new_train_features,\n",
    "          y=new_train_relevances,\n",
    "          group= qids_train)\n",
    "          #eval_set=[(new_train_features, new_train_relevances)]\n",
    "          #eval_group=qids_train,\n",
    "          #eval_at=[5, 10])\n",
    "          #eval_set=[(new_train_features, new_train_relevances),(new_test_features, new_test_relevances)],\n",
    "          #eval_group=[qids_train, qids_test],\n",
    "          #eval_at=[5, 10])\n",
    "    ranker.booster_.save_model('mode'+str(i)+'.txt')\n",
    "    b = time.time()\n",
    "    print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a9837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "for i in range(0, len(features_avg_test)):\n",
    "    test_feature = features_avg_test[i]\n",
    "    \n",
    "    features_avg_test = features_avg_test.reshape((features_avg_test.shape[0],-1))\n",
    "    \n",
    "    features_avg_train = features_avg_train.reshape((features_avg_train.shape[0], 220))\n",
    "    \n",
    "    dist , loc = get_nearest_neighbour(test_feature, features_avg_train,k)\n",
    "    #get the values of the second key(features) of the trainset\n",
    "    selected_train_set = features_avg_train[loc]\n",
    "    #get the nearest query ids and add them to this list\n",
    "    matching_train_qids = qid_avg_train[loc]\n",
    "    matching_train_qids = matching_train_qids.reshape(-1,1)\n",
    "    max_common = 0\n",
    "    group_id = 0\n",
    "    for k in range(0, len(train_ids_group)):\n",
    "        \n",
    "        commons_len = len(np.intersect1d(train_ids_group[k], matching_train_qids))\n",
    "        if(commons_len > max_common):\n",
    "            max_common = commons_len\n",
    "            group_id = k\n",
    "    \n",
    "    bst = lightgbm.Booster(model_file='mode'+str(group_id)+'.txt')\n",
    "    \n",
    "    test_data_ids = np.where(qid_all_test == qid_avg_test[i])[0]\n",
    "    new_test_qids = []\n",
    "    new_test_features = []\n",
    "    new_test_relevances = []\n",
    "    for k in range(0, len(test_data_ids)):\n",
    "            new_test_qids.append(qid_all_test[test_data_ids[k]])\n",
    "            new_test_features.append(features_all_test[test_data_ids[k]])\n",
    "            new_test_relevances.append(relevance_all_test[test_data_ids[k]])\n",
    "            \n",
    "    new_test_qids = np.array(new_test_qids)\n",
    "    new_test_features = np.array(new_test_features)\n",
    "    new_test_relevances = np.array(new_test_relevances)\n",
    "    \n",
    "    new_test_relevances = new_test_relevances.flatten() \n",
    "   \n",
    "    test_pred = bst.predict(new_test_features)\n",
    "    new_test_relevances = new_test_relevances.reshape((1, -1))\n",
    "    test_pred = test_pred.reshape((1, -1))\n",
    "    #print(test_pred)\n",
    "    print(ndcg_score(new_test_relevances, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4995583",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print((new_train_relevances[0:10]))\n",
    "\n",
    "#print(test_data_all)\n",
    "#print(test_data_all[test_data_all['q_id']==263])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df894337",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker.evals_result_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81cbb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a06229cffe52a28d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:58:15.916292408Z",
     "start_time": "2023-09-11T08:58:14.547580672Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import lightgbm as lightgbm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "774b3fa314830ccf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:58:25.445948931Z",
     "start_time": "2023-09-11T08:58:25.072937054Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/data/Python Projects/KNN-LETOR/valid_svm_features_avg.pkl', 'rb') as f:\n",
    "    preprocessed_train_cum_features = pickle.load(f) #main dict train with qids and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46687be3ff90a798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T08:58:34.036498085Z",
     "start_time": "2023-09-11T08:58:33.884522513Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/data/Python Projects/KNN-LETOR/test_svm_features_avg.pkl', 'rb') as f:\n",
    "    preprocessed_test_cum_features = pickle.load(f) #main dict with test qids and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1e882cc594d0082",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T09:00:35.262256708Z",
     "start_time": "2023-09-11T08:58:38.874160692Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/data/Python Projects/KNN-LETOR/valid_svm_features_rel.pkl', 'rb') as f:\n",
    "    train_data_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c452daa59f3064c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T09:02:39.039024364Z",
     "start_time": "2023-09-11T09:00:35.263272580Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/data/Python Projects/KNN-LETOR/test_svm_features_rel.pkl', 'rb') as f:\n",
    "    test_data_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80a13e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_cum_features= pd.DataFrame(preprocessed_train_cum_features)\n",
    "preprocessed_test_cum_features= pd.DataFrame(preprocessed_test_cum_features)\n",
    "train_data_all= pd.DataFrame(train_data_all)\n",
    "test_data_all= pd.DataFrame(test_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "091ef9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_avg_train = preprocessed_train_cum_features[\"q_id\"].tolist()\n",
    "features_avg_train = preprocessed_train_cum_features[\"features\"].tolist()\n",
    "qid_avg_train = np.vstack(qid_avg_train)\n",
    "features_avg_train = np.vstack(features_avg_train)\n",
    "\n",
    "qid_avg_test = preprocessed_test_cum_features[\"q_id\"].tolist()\n",
    "features_avg_test = preprocessed_test_cum_features[\"features\"].tolist()\n",
    "qid_avg_test = np.vstack(qid_avg_test)\n",
    "features_avg_test = np.vstack(features_avg_test)\n",
    "\n",
    "qid_all_train= train_data_all[\"q_id\"].tolist()\n",
    "relevance_all_train = train_data_all[\"relevance\"].tolist()\n",
    "features_all_train = train_data_all[\"features\"].tolist()\n",
    "qid_all_train = np.vstack(qid_all_train)\n",
    "relevance_all_train = np.vstack(relevance_all_train)\n",
    "features_all_train = np.vstack(features_all_train)\n",
    "\n",
    "qid_all_test = test_data_all[\"q_id\"].tolist()\n",
    "relevance_all_test = test_data_all[\"relevance\"].tolist()\n",
    "features_all_test = test_data_all[\"features\"].tolist()\n",
    "\n",
    "qid_all_test = np.vstack(qid_all_test)\n",
    "relevance_all_test = np.vstack(relevance_all_test)\n",
    "features_all_test = np.vstack(features_all_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae33fdc73b16fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e58bc081ddfb506",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19928\n",
      "[LightGBM] [Info] Number of data points in the train set: 81601, number of used features: 194\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA RTX A5000, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 93 dense feature groups (7.47 MB) transferred to GPU in 0.010575 secs. 1 sparse feature groups\n",
      "0.8528599739074707\n",
      "1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 20154\n",
      "[LightGBM] [Info] Number of data points in the train set: 82847, number of used features: 193\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA RTX A5000, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 94 dense feature groups (7.58 MB) transferred to GPU in 0.011839 secs. 1 sparse feature groups\n",
      "0.6957483291625977\n",
      "2\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19289\n",
      "[LightGBM] [Info] Number of data points in the train set: 57072, number of used features: 193\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA RTX A5000, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 107 dense feature groups (5.88 MB) transferred to GPU in 0.010884 secs. 1 sparse feature groups\n",
      "0.549241304397583\n",
      "3\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 17399\n",
      "[LightGBM] [Info] Number of data points in the train set: 16913, number of used features: 187\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA RTX A5000, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 93 dense feature groups (1.55 MB) transferred to GPU in 0.007934 secs. 1 sparse feature groups\n",
      "0.3235807418823242\n",
      "4\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 17652\n",
      "[LightGBM] [Info] Number of data points in the train set: 30448, number of used features: 192\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA RTX A5000, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 101 dense feature groups (3.02 MB) transferred to GPU in 0.008287 secs. 1 sparse feature groups\n",
      "0.41232919692993164\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "#find the nearest neighbour\n",
    "#key2: features\n",
    "#key1: qids\n",
    "def get_nearest_neighbour(test_q, features_avg_train,k):\n",
    "    knn = NearestNeighbors(n_neighbors=k)\n",
    "    features_avg_train = np.array(features_avg_train)\n",
    "    \n",
    "    knn.fit(features_avg_train)\n",
    "    test_q = test_q.reshape((1, -1))\n",
    "    dist_loc = knn.kneighbors(test_q)\n",
    "    dist = dist_loc[0]\n",
    "    loc = dist_loc[1]\n",
    "    return dist, loc\n",
    "#these have relevance score, qids, and features\n",
    "#key_0 = relevance score, key_qids, key2_features list\n",
    "\"\"\"\n",
    "test_unique_qids = []    # a list for getting the qids of tests\n",
    "for d in qid_avg_test:\n",
    "    if d not in test_unique_qids:\n",
    "        test_unique_qids.append(d)\n",
    "\"\"\"\n",
    "k= 300\n",
    "#qids_test = []\n",
    "train_ids_group =[]\n",
    "#len(qid_all_train)\n",
    "for i in range(0,5):\n",
    "    print(i)\n",
    "    #get features corresponding to the qids of the test\n",
    "    #qids_test.append(qid)\n",
    "    #test_q = #[d['features'] for d in preprocessed_train_cum_features if d.get('q_id') == key1_value]\n",
    "    #get the nearest neighbour of the test_q\n",
    "    train_feature_single = features_avg_train[i]\n",
    "    \n",
    "    #features_avg_test = features_avg_test.reshape((features_avg_test.shape[0],-1))\n",
    "    \n",
    "    features_avg_train = features_avg_train.reshape((features_avg_train.shape[0], 220))\n",
    "    \n",
    "    dist , loc = get_nearest_neighbour(train_feature_single, features_avg_train,k)\n",
    "    #get the values of the features of the trainset\n",
    "    selected_train_set = features_avg_train[loc]\n",
    "    #get the nearest query ids and add them to this list\n",
    "    matching_train_qids = qid_avg_train[loc]\n",
    "    matching_train_qids = matching_train_qids.reshape(-1,1)\n",
    "    new_train_features = []\n",
    "    new_train_qids = []\n",
    "    new_train_relevances = []\n",
    "    for qid in matching_train_qids:\n",
    "        train_data_ids = np.where(qid_all_train == qid)[0]\n",
    "        for k in range(0, len(train_data_ids)):\n",
    "            new_train_qids.append(qid_all_train[train_data_ids[k]])\n",
    "            new_train_features.append(features_all_train[train_data_ids[k]])\n",
    "            new_train_relevances.append(relevance_all_train[train_data_ids[k]])\n",
    "    \n",
    "    new_train_relevances = np.array(new_train_relevances)\n",
    "    new_train_features = np.array(new_train_features)\n",
    "    new_train_qids = np.array(new_train_qids)\n",
    "    \n",
    "    #add k-neareset ids to a list of ids \n",
    "    train_ids_group.append(matching_train_qids.flatten())\n",
    "    # Reshape \"relevance\" and \"qid\" arrays\n",
    "    new_train_relevances = new_train_relevances.reshape(-1)\n",
    "    new_train_qids = new_train_qids.reshape(-1)\n",
    "    #new_train_features = new_train_features.tolist()\n",
    "    # Concatenate arrays and list\n",
    "    #train_data = np.column_stack((new_train_relevances, new_train_qids, new_train_features))\n",
    "    data_train_tmp = {\n",
    "    'relevance': new_train_relevances,\n",
    "    'qid': new_train_qids,\n",
    "    'features': new_train_features.tolist()  # Convert features to a list of lists\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame\n",
    "    train_group = pd.DataFrame(data_train_tmp)\n",
    "    \n",
    "    qids_train = train_group.groupby(\"qid\")[\"qid\"].count().to_numpy()\n",
    "    #qids_test = test_group.groupby(\"qid\")[\"qid\"].count().to_numpy()\n",
    "    \n",
    "    new_train_relevances = new_train_relevances.astype(int)\n",
    "    #new_test_relevances = new_test_relevances.astype(int)\n",
    "    a = time.time()\n",
    "    ranker = lightgbm.LGBMRanker(\n",
    "                        objective=\"lambdarank\",\n",
    "                        boosting_type = \"gbdt\",\n",
    "                        n_estimators = 1,\n",
    "                        importance_type = \"gain\",\n",
    "                        force_col_wise=True,\n",
    "                        metric= \"ndcg\",\n",
    "                        num_leaves = 250,\n",
    "                        learning_rate = 0.05,\n",
    "                        max_depth = -1,\n",
    "                        device ='gpu',\n",
    "                        label_gain =[i for i in range((new_train_relevances.max()) + 1)])\n",
    "    \n",
    "    # Training the model\n",
    "    ranker.fit(\n",
    "          X=new_train_features,\n",
    "          y=new_train_relevances,\n",
    "          group= qids_train)\n",
    "          #eval_set=[(new_train_features, new_train_relevances)]\n",
    "          #eval_group=qids_train,\n",
    "          #eval_at=[5, 10])\n",
    "          #eval_set=[(new_train_features, new_train_relevances),(new_test_features, new_test_relevances)],\n",
    "          #eval_group=[qids_train, qids_test],\n",
    "          #eval_at=[5, 10])\n",
    "    ranker.booster_.save_model('mode'+str(i)+'.txt')\n",
    "    b = time.time()\n",
    "    print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e6b61e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_avg_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ndcg_score\n\u001b[1;32m      2\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mfeatures_avg_test\u001b[49m)):\n\u001b[1;32m      4\u001b[0m     test_feature \u001b[38;5;241m=\u001b[39m features_avg_test[i]\n\u001b[1;32m      6\u001b[0m     features_avg_test \u001b[38;5;241m=\u001b[39m features_avg_test\u001b[38;5;241m.\u001b[39mreshape((features_avg_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_avg_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "k = 1\n",
    "for i in range(0, len(features_avg_test)):\n",
    "    test_feature = features_avg_test[i]\n",
    "    \n",
    "    features_avg_test = features_avg_test.reshape((features_avg_test.shape[0],-1))\n",
    "    \n",
    "    features_avg_train = features_avg_train.reshape((features_avg_train.shape[0], 220))\n",
    "    \n",
    "    dist , loc = get_nearest_neighbour(test_feature, features_avg_train,k)\n",
    "    #get the values of the second key(features) of the trainset\n",
    "    selected_train_set = features_avg_train[loc]\n",
    "    #get the nearest query ids and add them to this list\n",
    "    matching_train_qids = qid_avg_train[loc]\n",
    "    matching_train_qids = matching_train_qids.reshape(-1,1)\n",
    "    print(train_ids_group)\n",
    "    asdsad\n",
    "    max_common = 0\n",
    "    group_id = 0\n",
    "    for j in range(0, len(train_ids_group)):\n",
    "        \n",
    "        commons_len = len(np.intersect1d(train_ids_group[j], matching_train_qids))\n",
    "        if(commons_len > max_common):\n",
    "            max_common = commons_len\n",
    "            group_id = j\n",
    "    \n",
    "    bst = lightgbm.Booster(model_file='mode'+str(group_id)+'.txt')\n",
    "    \n",
    "    test_data_ids = np.where(qid_all_test == qid_avg_test[i])[0]\n",
    "    new_test_qids = []\n",
    "    new_test_features = []\n",
    "    new_test_relevances = []\n",
    "    for k in range(0, len(test_data_ids)):\n",
    "            new_test_qids.append(qid_all_test[test_data_ids[k]])\n",
    "            new_test_features.append(features_all_test[test_data_ids[k]])\n",
    "            new_test_relevances.append(relevance_all_test[test_data_ids[k]])\n",
    "            \n",
    "    new_test_qids = np.array(new_test_qids)\n",
    "    new_test_features = np.array(new_test_features)\n",
    "    new_test_relevances = np.array(new_test_relevances)\n",
    "    \n",
    "    new_test_relevances = new_test_relevances.flatten() \n",
    "   \n",
    "    test_pred = bst.predict(new_test_features)\n",
    "    new_test_relevances = new_test_relevances.reshape((1, -1))\n",
    "    test_pred = test_pred.reshape((1, -1))\n",
    "    #print(test_pred)\n",
    "    print(ndcg_score(new_test_relevances, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b36513df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     q_id  relevance                                           features\n",
      "0     263        0.0  [1.0, 1444.0, 2.0, 10.0, 53.0, 120.0, 3.0, 556...\n",
      "1     263        0.0  [1000000.0, 1004.0, 0.0, 0.0, 53.0, 10.0, 1.0,...\n",
      "2     263        0.0  [1000000.0, 1004.0, 0.0, 0.0, 53.0, 130.0, 1.0...\n",
      "3     263        0.0  [1000000.0, 1011.0, 0.0, 0.0, 53.0, 106.0, 1.0...\n",
      "4     263        0.0  [1000000.0, 1017.0, 0.0, 1.0, 53.0, 50.0, 1.0,...\n",
      "..    ...        ...                                                ...\n",
      "994   263        0.0  [9589.0, 984.0, 2.0, 63.0, 53.0, 130.0, 2.0, 5...\n",
      "995   263        0.0  [965084.0, 727.0, 0.0, 0.0, 53.0, 99.0, 1.0, 5...\n",
      "996   263        0.0  [967780.0, 1144.0, 0.0, 7.0, 53.0, 130.0, 2.0,...\n",
      "997   263        0.0  [993637.0, 899.0, 2.0, 5.0, 53.0, 100.0, 1.0, ...\n",
      "998   263        0.0  [996789.0, 626.0, 0.0, 0.0, 53.0, 95.0, 1.0, 5...\n",
      "\n",
      "[999 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#print((new_train_relevances[0:10]))\n",
    "\n",
    "#print(test_data_all)\n",
    "#print(test_data_all[test_data_all['q_id']==263])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "df894337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': OrderedDict([('ndcg@5', [0.7490828397399896]),\n",
       "              ('ndcg@10', [0.7920798893578783])]),\n",
       " 'valid_1': OrderedDict([('ndcg@5', [0.8687949224876582]),\n",
       "              ('ndcg@10', [0.7751482523375255])])}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker.evals_result_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81cbb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
